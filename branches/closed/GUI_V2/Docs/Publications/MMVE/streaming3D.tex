\section{3D streaming}
\subsection{context}
A user generated virtual environments is in perpetual evolution, requiring regular updates of the 3D models composing it. The actual solution consists in transmitting on the fly to the visualization client the different contents such as 3D models, images and videos. Streaming various media types, as opposed to the classical "download and play" paradigm, is already a widely used method, when the available bitrate is compatible with the quality required at rendering. 
In the case of 3D environments, the regions of interest are naturally the ones through which the user is roaming at a given time. As opposed to audio or video, the use of 3D content implies that the user is embedded within the media, and interacts with it. Hence, it is required that the environment be refined in real-time, according to the actual navigation and various interactions. The regions around the observer have naturally to be best refined, while the non-visible or distant regions can only be transmitted in coarser versions, ready to be refined when they become regions of interest.

\subsection{Adaptive 3D streaming}
In order to implement this so-called "view-dependency", several solutions can be considered. First, space partitionning is already a widely used method to filter the required contents according to the current viewpoint. The virtual environments are usually partitionned in cells thanks to a regular grid. When an avatar navigate into a cell, the server, or in our case peers, sent to the visualization client the contents present in this one. To reduce the amount of data to transmit over the network, 3D objects are generally represented using non-continuous level of details. Thus, a well-detailed representation of 3D contents close to the view point are sent to the client. Finally, a pre-loading of nearby cells can be done for anticipation.\\
This filtering solution, thanks to which visualized contents are progressively transmitted to visualization nodes according to their viewpoints, could be considered as an answer to adaptive streaming requirements. Moreover, the mapping of the Raynet layer into the virtual world provide us directly a space partitionning. But non-continuous level of details have the drawback to contain redundant information (as a level of detail does not use information from a lower level), and generally bring about popping effects when a model swap from one level of detail to the next one. For these reasons, the Solipsis framework supports continuous level of detail representations for textures, 3D models and animation.
\subsection{On-Demand 3D Streaming }
Autonomy of peers is a essential requirement to obtain a full scalable architecture. As explain previously, actual centralized architectures for virtual environments execute a significant amount of computation on the server-side.  Generally, clients have not a global knowledge of the scene, unlike servers that are mostly connected to a database containing a complete description of the 3D world. For these reasons, most of process such as level of detail selection,  collision detection, physics computation and animation are performed on the server side. Unfortunately, servers becomes quickly overloaded as the number of connected client increases. Only, as shown in \cite{Cavagna}, the complete model of the world is not necessary to compute the level of detail selection (the case of collision detections will be discussed in \ref{collision detection}). For 3D contents, Cavagna et al. disjoined the information required for rendering (3D model, textures, etc) to data used to select level of details. Thus, descriptors introduced in \ref{} are extended to include selection informations.     

   
